@book{wickham_r_2023,
  title = {R for {{Data Science}}},
  author = {Wickham, Hadley and {\c C}etinkaya-Rundel, Mine and Grolemund, Garrett},
  year = {2023},
  edition = {2}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2024-04-17},
  abstract = {Empirically analyzing empirical evidence                            One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts               et al.               describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.                                         Science               , this issue               10.1126/science.aac4716                        ,              A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.           ,                             INTRODUCTION               Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.                                         RATIONALE               There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.                                         RESULTS                                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and                 P                 values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (                 M                 r                 = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (                 M                 r                 = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (                 P                 {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.                                                        CONCLUSION                                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original                 P                 value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.                              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.                                                   Original study effect size versus replication effect size (correlation coefficients).                   Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.                                                                         ,              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  copyright = {http://www.sciencemag.org/about/science-licenses-journal-article-reuse},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/KEP964XF/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}
@article{bouter_what_2020,
  title = {What {{Research Institutions Can Do}} to {{Foster Research Integrity}}},
  author = {Bouter, Lex},
  year = {2020},
  month = aug,
  journal = {Science and Engineering Ethics},
  volume = {26},
  number = {4},
  pages = {2363--2369},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-020-00178-5},
  urldate = {2023-03-10},
  abstract = {In many countries attention for fostering research integrity started with a misconduct case that got a lot of media exposure. But there is an emerging consensus that questionable research practices are more harmful due to their high prevalence. QRPs have in common that they can help to make study results more exciting, more positive and more statistically significant. That makes them tempting to engage in. Research institutions have the duty to empower their research staff to steer away from QRPs and to explain how they realize that in a Research Integrity Promotion Plan. Avoiding perverse incentives in assessing researchers for career advancement is an important element in that plan. Research institutions, funding agencies and journals should make their research integrity policies as evidence-based as possible. The dilemmas and distractions researchers face are real and universal. We owe it to society to collaborate and to do our utmost best to prevent QRPs and to foster research integrity.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/DXPHLHEH/Bouter - 2020 - What Research Institutions Can Do to Foster Resear.pdf}
}

@incollection{prinstein_open_2022,
  title = {An {{Open Science Workflow}} for {{More Credible}}, {{Rigorous Research}}},
  booktitle = {The {{Portable Mentor}}},
  author = {Corker, Katherine S.},
  editor = {Prinstein, Mitchell J.},
  year = {2022},
  month = jul,
  edition = {3},
  pages = {197--216},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108903264.012},
  urldate = {2024-04-29},
  isbn = {978-1-108-90326-4 978-1-108-84242-6 978-1-108-79438-1},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/P8DK6X5E/Corker - 2022 - An Open Science Workflow for More Credible, Rigoro.pdf}
}
@book{bryan_chapter_nodate,
  title = {Chapter 2 {{R}} Basics and Workflows {\textbar} {{STAT}} 545},
  author = {Bryan, Jenny and TAs, The STAT 545},
  urldate = {2024-05-06},
  abstract = {Chapter 2 R basics and workflows {\textbar} STAT 545: Data wrangling, exploration, and analysis with R.}
}

@misc{noauthor_using_2024,
  title = {Using {{RStudio Projects}}},
  year = {2024},
  month = apr,
  journal = {Posit Support},
  urldate = {2024-05-06},
  abstract = {Using Projects RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents. Creating Project...},
  howpublished = {https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects},
  langid = {american},
  file = {/Users/danielapalleschi/Zotero/storage/GBLGI7MT/200526207-Using-RStudio-Projects.html}
}

@misc{noauthor_what_nodate-1,
  title = {What {{They Forgot}} to {{Teach You About R}}},
  author = {Bryan, Jennifer and Hester, Jim and Pileggi, Shannon and Aja, David E.},
  urldate = {2024-05-06},
  howpublished = {https://rstats.wtf/},
  file = {/Users/danielapalleschi/Zotero/storage/RLVTQAMK/rstats.wtf.html}
}


@article{cruwell_seven_2019,
  title = {Seven {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {Seven {{Easy Steps}} to {{Open Science}}},
  author = {Cr{\"u}well, Sophia and Van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse C. and Orben, Amy and Parsons, Sam and {Schulte-Mecklenbeck}, Michael},
  year = {2019},
  month = oct,
  journal = {Zeitschrift f{\"u}r Psychologie},
  volume = {227},
  number = {4},
  pages = {237--248},
  issn = {2190-8370, 2151-2604},
  doi = {10.1027/2151-2604/a000387},
  urldate = {2024-04-15},
  abstract = {The open science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz, Gronau, Dablander, Edelsbrunner, and Baribault (2018). Written for researchers and students -- particularly in psychological science -- it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/RV5VZALC/Crüwell et al. - 2019 - Seven Easy Steps to Open Science An Annotated Rea.pdf}
}

@article{guttinger_limits_2020,
  title = {The Limits of Replicability},
  author = {Guttinger, Stephan},
  year = {2020},
  month = may,
  journal = {European Journal for Philosophy of Science},
  volume = {10},
  number = {2},
  pages = {10},
  issn = {1879-4912, 1879-4920},
  doi = {10.1007/s13194-019-0269-1},
  urldate = {2023-03-10},
  abstract = {Discussions about a replicability crisis in science have been driven by the normative claim that all of science should be replicable and the empirical claim that most of it isn't. Recently, such crisis talk has been challenged by a new localism, which argues a) that serious problems with replicability are not a general occurrence in science and b) that replicability itself should not be treated as a universal standard. The goal of this article is to introduce this emerging strand of the debate and to discuss some of its implications and limitations. I will in particular highlight the issue of demarcation that localist accounts have to address, i.e. the question of how we can distinguish replicable science from disciplines where replicability does not apply.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/2J6DF99P/Guttinger - 2020 - The limits of replicability.pdf}
}

@article{ioannidis_why_2005,
  title = {Why Most Published Research Findings Are False},
  author = {Ioannidis, John P.A.},
  year = {2005},
  journal = {PLoS Med},
  volume = {2},
  number = {8},
  pages = {2--8},
  issn = {15491277},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  isbn = {9783319513584},
  pmid = {16060722},
  file = {/Users/danielapalleschi/Zotero/storage/W7EEEEIX/Ioannidis - 2005 - Why most published research findings are false.pdf}
}

@article{kathawalla_easing_2021,
  title = {Easing {{Into Open Science}}: {{A Guide}} for {{Graduate Students}} and {{Their Advisors}}},
  shorttitle = {Easing {{Into Open Science}}},
  author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
  year = {2021},
  month = jan,
  journal = {Collabra: Psychology},
  volume = {7},
  number = {1},
  pages = {18684},
  issn = {2474-7394},
  doi = {10.1525/collabra.18684},
  urldate = {2023-03-09},
  abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/3MM594QZ/Kathawalla et al. - 2021 - Easing Into Open Science A Guide for Graduate Stu.pdf}
}

@techreport{kobrock_assessing_2022,
  type = {Preprint},
  title = {Assessing the Replication Landscape in Experimental Linguistics},
  author = {Kobrock, Kristina and Roettger, Timo Benjamin},
  year = {2022},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/fzngs},
  urldate = {2023-03-31},
  abstract = {Replications are an integral part of cumulative experimental science. Yet many scientific disciplines do not replicate enough because novel confirmatory findings are valued over direct replications. To provide a systematic assessment of the replication landscape in experimental linguistics, the present study estimated replication rates for over 50.000 articles across 98 journals. We used automatic string matching using the Web of Science combined with in-depth manual inspections of 210 papers. The median rate of mentioning the search string "replicat*" was as low as 1.6\%. Subsequent manual analyses revealed that only eight of these were direct replications, i.e. studies that arrive at the same scientific conclusions as an initial study by using exactly the same methodology. Moreover, only 1 in 1600 experimental linguistic studies reports a direct replication performed by independent researchers. We conclude that, similar to neighboring disciplines, experimental linguistics does not replicate enough.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/I4KRKTLG/Kobrock and Roettger - 2022 - Assessing the replication landscape in experimenta.pdf}
}

@article{kohrs_eleven_2023,
  title = {Eleven Strategies for Making Reproducible Research and Open Science Training the Norm at Research Institutions},
  author = {Kohrs, Friederike E and Auer, Susann and {Bannach-Brown}, Alexandra and Fiedler, Susann and Haven, Tamarinde Laura and Heise, Verena and Holman, Constance and Azevedo, Flavio and Bernard, Ren{\'e} and Bleier, Armin and B{\"o}ssel, Nicole and Cahill, Brian Patrick and Castro, Leyla Jael and Ehrenhofer, Adrian and Eichel, Kristina and Frank, Maximillian and Frick, Claudia and Friese, Malte and G{\"a}rtner, Anne and Gierend, Kerstin and Gr{\"u}ning, David Joachim and Hahn, Lena and H{\"u}lsemann, Maren and Ihle, Malika and Illius, Sabrina and K{\"o}nig, Laura and K{\"o}nig, Matthias and Kulke, Louisa and Kutlin, Anton and Lammers, Fritjof and Mehler, David Ma and Miehl, Christoph and {M{\"u}ller-Alcazar}, Anett and Neuendorf, Claudia and Niemeyer, Helen and Pargent, Florian and Peikert, Aaron and Pfeuffer, Christina U and Reinecke, Robert and R{\"o}er, Jan Philipp and Rohmann, Jessica L and {S{\'a}nchez-T{\'o}jar}, Alfredo and Scherbaum, Stefan and Sixtus, Elena and Spitzer, Lisa and Stra{\ss}burger, Vera Maren and Weber, Marcel and Whitmire, Clarissa J and Zerna, Josephine and Zorbek, Dilara and Zumstein, Philipp and Weissgerber, Tracey L},
  year = {2023},
  month = nov,
  journal = {eLife},
  volume = {12},
  pages = {e89736},
  issn = {2050-084X},
  doi = {10.7554/eLife.89736},
  urldate = {2023-11-24},
  abstract = {Reproducible research and open science practices have the potential to accelerate scientific progress by allowing others to reuse research outputs, and by promoting rigorous research that is more likely to yield trustworthy results. However, these practices are uncommon in many fields, so there is a clear need for training that helps and encourages researchers to integrate reproducible research and open science practices into their daily work. Here, we outline eleven strategies for making training in these practices the norm at research institutions. The strategies, which emerged from a virtual brainstorming event organized in collaboration with the German Reproducibility Network, are concentrated in three areas: (i) adapting research assessment criteria and program requirements; (ii) training; (iii) building communities. We provide a brief overview of each strategy, offer tips for implementation, and provide links to resources. We also highlight the importance of allocating resources and monitoring impact. Our goal is to encourage researchers -- in their roles as scientists, supervisors, mentors, instructors, and members of curriculum, hiring or evaluation committees -- to think creatively about the many ways they can promote reproducible research and open science practices in their institutions.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/W95CV75A/Kohrs et al. - 2023 - Eleven strategies for making reproducible research.pdf}
}

@article{laurinavichyute_share_2022,
  title = {Share the Code, Not Just the Data: {{A}} Case Study of the Reproducibility of Articles Published in the {{Journal}} of {{Memory}} and {{Language}} under the Open Data Policy},
  author = {Laurinavichyute, Anna and Yadav, Himanshu and Vasishth, Shravan},
  year = {2022},
  journal = {Journal of Memory and Language},
  volume = {125},
  pages = {12},
  abstract = {In 2019 the Journal of Memory and Language instituted an open data and code policy; this policy requires that, as a rule, code and data be released at the latest upon publication. How effective is this policy? We compared 59 papers published before, and 59 papers published after, the policy took effect. After the policy was in place, the rate of data sharing increased by more than 50\%. We further looked at whether papers published under the open data policy were reproducible, in the sense that the published results should be possible to regenerate given the data, and given the code, when code was provided. For 8 out of the 59 papers, data sets were inaccessible. The reproducibility rate ranged from 34\% to 56\%, depending on the reproducibility criteria. The strongest predictor of whether an attempt to reproduce would be successful is the presence of the analysis code: it increases the probability of reproducing reported results by almost 40\%. We propose two simple steps that can increase the reproducibility of published papers: share the analysis code, and attempt to reproduce one's own analysis using only the shared materials.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/ISZ9VFQU/Laurinavichyute - 2022 - Share the code, not just the data A case study of.pdf}
}

@article{munafo_manifesto_2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie Du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {0021},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  urldate = {2023-06-21},
  abstract = {Abstract             Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/4VBT6YR9/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}

@article{nieuwland_large-scale_2018,
  title = {Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension},
  author = {Nieuwland, Mante S and {Politzer-Ahles}, Stephen and Heyselaar, Evelien and Segaert, Katrien and Darley, Emily and Kazanina, Nina and Von Grebmer Zu Wolfsthurn, Sarah and Bartolozzi, Federica and Kogan, Vita and Ito, Aine and M{\'e}zi{\`e}re, Diane and Barr, Dale J and Rousselet, Guillaume A and Ferguson, Heather J and {Busch-Moreno}, Simon and Fu, Xiao and Tuomainen, Jyrki and Kulakova, Eugenia and Husband, E Matthew and Donaldson, David I and Koh{\'u}t, Zdenko and Rueschemeyer, Shirley-Ann and Huettig, Falk},
  year = {2018},
  month = apr,
  journal = {eLife},
  volume = {7},
  pages = {e33468},
  issn = {2050-084X},
  doi = {10.7554/eLife.33468},
  urldate = {2023-03-09},
  abstract = {Do people routinely pre-activate the meaning and even the phonological form of upcoming words? The most acclaimed evidence for phonological prediction comes from a 2005 Nature Neuroscience publication by DeLong, Urbach and Kutas, who observed a graded modulation of electrical brain potentials (N400) to nouns and preceding articles by the probability that people use a word to continue the sentence fragment (`cloze'). In our direct replication study spanning 9 laboratories (N=334), pre-registered replication-analyses and exploratory Bayes factor analyses successfully replicated the noun-results but, crucially, not the article-results. Pre-registered single-trial analyses also yielded a statistically significant effect for the nouns but not the articles. Exploratory Bayesian single-trial analyses showed that the article-effect may be non-zero but is likely far smaller than originally reported and too small to observe without very large sample sizes. Our results do not support the view that readers routinely pre-activate the phonological form of predictable words.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/V3YDEFEN/Nieuwland et al. - 2018 - Large-scale replication study reveals a limit on p.pdf}
}

@article{nosek_preregistration_2018,
  title = {The Preregistration Revolution},
  author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2600--2606},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708274114},
  urldate = {2024-04-10},
  abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes---a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/MX24569R/Nosek et al. - 2018 - The preregistration revolution.pdf}
}

@article{peels_replicability_2019,
  title = {Replicability and Replication in the Humanities},
  author = {Peels, Rik},
  year = {2019},
  month = dec,
  journal = {Research Integrity and Peer Review},
  volume = {4},
  number = {1},
  pages = {2},
  issn = {2058-8615},
  doi = {10.1186/s41073-018-0060-4},
  urldate = {2023-03-10},
  abstract = {A large number of scientists and several news platforms have, over the last few years, been speaking of a replication crisis in various academic disciplines, especially the biomedical and social sciences. This paper answers the novel question of whether we should also pursue replication in the humanities. First, I create more conceptual clarity by defining, in addition to the term ``humanities,'' various key terms in the debate on replication, such as ``reproduction'' and ``replicability.'' In doing so, I pay attention to what is supposed to be the object of replication: certain studies, particular inferences, of specific results. After that, I spell out three reasons for thinking that replication in the humanities is not possible and argue that they are unconvincing. Subsequently, I give a more detailed case for thinking that replication in the humanities is possible. Finally, I explain why such replication in the humanities is not only possible, but also desirable.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/CLMFM22U/Peels - 2019 - Replicability and replication in the humanities.pdf}
}

@misc{peikert_reproducible_2019,
  title = {A {{Reproducible Data Analysis Workflow}} with {{R Markdown}}, {{Git}}, {{Make}}, and {{Docker}}},
  author = {Peikert, Aaron and Brandmaier, Andreas Markus},
  year = {2019},
  month = nov,
  doi = {10.31234/osf.io/8xzqy},
  urldate = {2024-04-12},
  abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles, and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), 2) the analysis exactly reproduces at a later point in time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible. While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Combining containerization, dependence management, version management, and dynamic document generation, the proposed workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/SKS4QVBI/Peikert and Brandmaier - 2019 - A Reproducible Data Analysis Workflow with R Markd.pdf}
}

@article{peikert_reproducible_2021,
  title = {Reproducible {{Research}} in {{R}}: {{A Tutorial}} on {{How}} to {{Do}} the {{Same Thing More Than Once}}},
  shorttitle = {Reproducible {{Research}} in {{R}}},
  author = {Peikert, Aaron and Van Lissa, Caspar J. and Brandmaier, Andreas M.},
  year = {2021},
  month = dec,
  journal = {Psych},
  volume = {3},
  number = {4},
  pages = {836--867},
  issn = {2624-8611},
  doi = {10.3390/psych3040053},
  urldate = {2024-04-12},
  abstract = {Computational reproducibility is the ability to obtain identical results from the same data with the same computer code. It is a building block for transparent and cumulative science because it enables the originator and other researchers, on other computers and later in time, to reproduce and thus understand how results came about, while avoiding a variety of errors that may lead to erroneous reporting of statistical and computational results. In this tutorial, we demonstrate how the R package repro supports researchers in creating fully computationally reproducible research projects with tools from the software engineering community. Building upon this notion of fully automated reproducibility, we present several applications including the preregistration of research plans with code (Preregistration as Code, PAC). PAC eschews all ambiguity of traditional preregistration and offers several more advantages. Making technical advancements that serve reproducibility more widely accessible for researchers holds the potential to innovate the research process and to help it become more productive, credible, and reliable.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/RB87QZR6/Peikert et al. - 2021 - Reproducible Research in R A Tutorial on How to D.pdf}
}

@article{penders_rinse_2019,
  title = {Rinse and {{Repeat}}: {{Understanding}} the {{Value}} of {{Replication}} across {{Different Ways}} of {{Knowing}}},
  shorttitle = {Rinse and {{Repeat}}},
  author = {{Penders} and {Holbrook} and {de Rijcke}},
  year = {2019},
  month = jul,
  journal = {Publications},
  volume = {7},
  number = {3},
  pages = {52},
  issn = {2304-6775},
  doi = {10.3390/publications7030052},
  urldate = {2023-03-10},
  abstract = {The increasing pursuit of replicable research and actual replication of research is a political project that articulates a very specific technology of accountability for science. This project was initiated in response to concerns about the openness and trustworthiness of science. Though applicable and valuable in many fields, here we argue that this value cannot be extended everywhere, since the epistemic content of fields, as well as their accountability infrastructures, differ. Furthermore, we argue that there are limits to replicability across all fields; but in some fields, including parts of the humanities, these limits severely undermine the value of replication to account for the value of research.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/IUJ28I3J/Penders et al. - 2019 - Rinse and Repeat Understanding the Value of Repli.pdf}
}

@article{penders_sciences_2020,
  title = {Science's Moral Economy of Repair: {{Replication}} and the Circulation of Reference},
  shorttitle = {Science's Moral Economy of Repair},
  author = {Penders, Bart and {de Rijcke}, Sarah and Holbrook, J. Britt},
  year = {2020},
  month = feb,
  journal = {Accountability in Research},
  volume = {27},
  number = {2},
  pages = {107--113},
  issn = {0898-9621, 1545-5815},
  doi = {10.1080/08989621.2020.1720659},
  urldate = {2023-03-10},
  abstract = {Responding to the so-called reproducibility crisis, various disciplines have proposed -- and some have implemented --changes in research practices and policies. These changes have been aligned with a restricted and rather uniform conceptualization of what science is, and knowledge is made. However, knowledge-making is not a uniform affair. Here, we reflect on a salient fault line running through Wissenschaft (the whole of academic knowledge making, spanning the sciences and humanities), grounded in the relationship between the acts of research and writing, separating research as reporting from research as writing. We do so to demonstrate that replication and replicability cannot be treated as uniformly applicable and that assessment and improvement of research quality invites various tools and strategies. Among those, replication is important, but not omnipresent. Considering these other tools and strategies in context allows us to situate the value of replication for knowledge making as a whole.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/75F6ILTF/Penders et al. - 2020 - Science’s moral economy of repair Replication and.pdf}
}

@article{roettger_preregistration_2021,
  title = {Preregistration in Experimental Linguistics: {{Applications}}, Challenges, and Limitations},
  author = {Roettger, Timo B.},
  year = {2021},
  journal = {Linguistics},
  volume = {59},
  number = {5},
  pages = {1227--1249},
  issn = {00243949},
  doi = {10.1515/ling-2019-0048},
  abstract = {The current publication system neither incentivizes publishing null results nor direct replication attempts, which biases the scientific record toward novel findings that appear to support presented hypotheses (referred to as "publication bias"). Moreover, flexibility in data collection, measurement, and analysis (referred to as "researcher degrees of freedom") can lead to overconfident beliefs in the robustness of a statistical relationship. One way to systematically decrease publication bias and researcher degrees of freedom is preregistration. A preregistration is a time-stamped document that specifies how data is to be collected, measured, and analyzed prior to data collection. While preregistration is a powerful tool to reduce bias, it comes with certain challenges and limitations which have to be evaluated for each scientific discipline individually. This paper discusses the application, challenges and limitations of preregistration for experimental linguistic research.},
  keywords = {confirmatory,exploratory,preregistration,publication bias,registered report,researcher degrees of freedom},
  file = {/Users/danielapalleschi/Zotero/storage/QG264CUR/Roettger_2021.pdf}
}

@article{roettger_reproducible_2022,
  title = {Reproducible Research Practices and Transparency in Linguistics},
  author = {Roettger, Timo and Bochynska, Agata and Buchanan, Erin and Casillas, Joseph and Halfacre, Caitlin and Keeble, Liam and R{\"o}thlisberger, Melanie and Champagne, Irys-Am{\'e}lie and Chen, Kaidi},
  year = {2022},
  month = aug,
  publisher = {{Open Science Framework}},
  doi = {10.17605/OSF.IO/J2Q5P},
  urldate = {2023-03-09},
  abstract = {Scientific studies of language span across many disciplines and provide evidence for social, cultural, cognitive, technological, and biomedical studies of human nature and behavior. By becoming increasingly empirical and quantitative, linguistics has been facing challenges and limitations of the scientific practices that pose barriers to reproducibility and replicability. One of the proposed solutions to the widely acknowledged reproducibility and replicability crisis has been the implementation of transparency practices, e.g. open access publishing, preregistrations, sharing study materials, data, and analyses, performing study replications and declaring conflicts of interest. Here, we have assessed the prevalence of these practices in randomly sampled 600 journal articles from linguistics across two time points. In line with similar studies in other disciplines, we found a moderate amount of articles published open access, but overall low rates of sharing materials, data, and protocols, no preregistrations, very few replications and low rates of conflict of interest reports. These low rates have not increased noticeably between 2008/2009 and 2018/2019, pointing to remaining barriers and slow adoption of open and reproducible research practices in linguistics. As linguistics has not yet firmly established transparency and reproducibility as guiding principles in research, we provide recommendations and solutions for facilitating the adoption of these practices.},
  collaborator = {Open Science Framework},
  copyright = {CC-By Attribution 4.0 International},
  langid = {english},
  keywords = {Arts and Humanities,linguistics,open access,open science,reproducibility,Social and Behavioral Sciences,transparency},
  file = {/Users/danielapalleschi/Zotero/storage/KDXH92D7/Roettger et al. - 2022 - Transparency in Linguistics.pdf}
}

@article{roettger_researcher_2019,
  title = {Researcher Degrees of Freedom in Phonetic Research},
  author = {Roettger, Timo B.},
  year = {2019},
  month = jan,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {10},
  number = {1},
  pages = {1},
  issn = {1868-6354},
  doi = {10.5334/labphon.147},
  urldate = {2024-03-01},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/FVUNI6C7/Roettger - 2019 - Researcher degrees of freedom in phonetic research.pdf}
}

@article{sonning_replication_2021,
  title = {The Replication Crisis, Scientific Revolutions, and Linguistics},
  author = {S{\"o}nning, Lukas and Werner, Valentin},
  year = {2021},
  month = sep,
  journal = {Linguistics},
  volume = {59},
  number = {5},
  pages = {1179--1206},
  issn = {0024-3949, 1613-396X},
  doi = {10.1515/ling-2019-0045},
  urldate = {2023-03-10},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/R8FVJS53/Sönning and Werner - 2021 - The replication crisis, scientific revolutions, an.pdf}
}

@misc{tierney_realistic_2020,
  title = {A {{Realistic Guide}} to {{Making Data Available Alongside Code}} to {{Improve Reproducibility}}},
  author = {Tierney, Nicholas J. and Ram, Karthik},
  year = {2020},
  month = feb,
  number = {arXiv:2002.11626},
  eprint = {2002.11626},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-12-08},
  abstract = {Data makes science possible. Sharing data improves visibility, and makes the research process transparent. This increases trust in the work, and allows for independent reproduction of results. However, a large proportion of data from published research is often only available to the original authors. Despite the obvious benefits of sharing data, and scientists' advocating for the importance of sharing data, most advice on sharing data discusses its broader benefits, rather than the practical considerations of sharing. This paper provides practical, actionable advice on how to actually share data alongside research. The key message is sharing data falls on a continuum, and entering it should come with minimal barriers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Digital Libraries},
  file = {/Users/danielapalleschi/Zotero/storage/2HZWIJBA/Tierney and Ram - 2020 - A Realistic Guide to Making Data Available Alongsi.pdf;/Users/danielapalleschi/Zotero/storage/ENJQ2CIC/2002.html}
}
@article{gelman_statistical_nodate,
  title = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2014},
  journal = {American Scientist},
  volume = {102},
  number = {6},
  pages = {460--465},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/YGPJWFM4/Gelman and Loken - The statistical crisis in science data-dependent .pdf}
}
@article{wasserstein_asa_2016,
  title = {The {{ASA Statement}} on {\emph{p}} -{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on {\emph{p}} -{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2024-03-11},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/IZTWI8UL/Wasserstein and Lazar - 2016 - The ASA Statement on p -Values Context, Pr.pdf}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} `` {\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2024-03-11},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/J5ZTZJ2G/Wasserstein et al. - 2019 - Moving to a World Beyond “ p  0.05”.pdf}
}
@article{quintana_replication_2021,
  title = {Replication Studies for Undergraduate Theses to Improve Science and Education},
  author = {Quintana, Daniel S.},
  year = {2021},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {9},
  pages = {1117--1118},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01192-8},
  urldate = {2024-04-18},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/ZUTWR3MB/Quintana - 2021 - Replication studies for undergraduate theses to im.pdf}
}
@article{hardwicke_populating_2018,
  title = {Populating the {{Data Ark}}: {{An}} Attempt to Retrieve, Preserve, and Liberate Data from the Most Highly-Cited Psychology and Psychiatry Articles},
  shorttitle = {Populating the {{Data Ark}}},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  editor = {Wicherts, Jelte M.},
  year = {2018},
  month = aug,
  journal = {PLOS ONE},
  volume = {13},
  number = {8},
  pages = {e0201856},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0201856},
  urldate = {2024-04-27},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/LG227N75/Hardwicke and Ioannidis - 2018 - Populating the Data Ark An attempt to retrieve, p.pdf}
}
@article{Knuth_literate_1984,
  title = {Literate Programming},
  author = {Knuth, Donald},
  year = {1984},
  journal = {The computer journal},
  volume = {27},
  number = {2},
  pages = {97--111},
  urldate = {2024-04-29},
  file = {/Users/danielapalleschi/Zotero/storage/VE5KNQIB/270097.pdf}
}

@article{bochynska_reproducible_2023,
  title = {Reproducible Research Practices and Transparency across Linguistics},
  author = {Bochynska, Agata and Keeble, Liam and Halfacre, Caitlin and Casillas, Joseph V. and Champagne, Irys-Am{\'e}lie and Chen, Kaidi and R{\"o}thlisberger, Melanie and Buchanan, Erin M. and Roettger, Timo B.},
  year = {2023},
  month = nov,
  journal = {Glossa Psycholinguistics},
  volume = {2},
  number = {1},
  issn = {2767-0279},
  doi = {10.5070/G6011239},
  urldate = {2024-04-27},
  abstract = {Scientific studies of language span across many disciplines and provide evidence for social,\&nbsp; cultural, cognitive, technological, and biomedical studies of human nature and behavior. As it becomes increasingly empirical and quantitative, linguistics has been facing challenges and limitations of the scientific practices that pose barriers to reproducibility and replicability. One of the\&nbsp; proposed solutions to the widely acknowledged reproducibility and replicability crisis has been the implementation of transparency practices,\&nbsp; e.g., open access publishing, preregistrations, sharing study materials, data, and analyses, performing study replications, and declaring conflicts of interest. Here, we have assessed the prevalence of these practices in 600 randomly sampled journal articles from linguistics across two time points. In line with similar studies in other disciplines, we found that 35\% of the articles were published open access and the rates of sharing materials, data, and protocols were below 10\%. None of the articles reported preregistrations, 1\% reported replications, and 10\% had conflict of interest statements. These rates have not increased noticeably between 2008/2009 and 2018/2019, pointing to remaining barriers and the slow adoption of open and reproducible research practices in linguistics. To facilitate adoption of these practices, we provide a range of recommendations and solutions for implementing transparency and improving reproducibility of research in linguistics.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/PGLCZKLM/Bochynska et al. - 2023 - Reproducible research practices and transparency a.pdf}
}

@article{p_simmons_preregistration_2021,
  title = {Pre-registration: {{Why}} and {{How}}},
  shorttitle = {Pre-registration},
  author = {P. Simmons, Joseph and D. Nelson, Leif and Simonsohn, Uri},
  year = {2021},
  month = jan,
  journal = {Journal of Consumer Psychology},
  volume = {31},
  number = {1},
  pages = {151--162},
  issn = {1057-7408, 1532-7663},
  doi = {10.1002/jcpy.1208},
  urldate = {2024-04-17},
  abstract = {In this article, we (1) discuss the reasons why pre-registration is a good idea, both for the field and individual researchers, (2) respond to arguments against pre-registration, (3) describe how to best write and review a pre-registration, and (4) comment on pre-registration's rapidly accelerating popularity. Along the way, we describe the (big) problem that pre-registration can solve (i.e., false positives caused by p-hacking), while also offering viable solutions to the problems that pre-registration cannot solve (e.g., hidden confounds or fraud). Pre-registration does not guarantee that every published finding will be true, but without it you can safely bet that many more will be false. It is time for our field to embrace pre-registration, while taking steps to ensure that it is done right.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/34GUELN8/P. Simmons et al. - 2021 - Pre‐registration Why and How.pdf}
}

@article{simmons_false-positive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  urldate = {2024-04-16},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/LZT6TVJR/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{vasishth_how_2021,
  title = {How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis},
  author = {Vasishth, Shravan and Gelman, Andrew},
  year = {2021},
  journal = {Linguistics},
  volume = {59},
  number = {5},
  pages = {1311--1342},
  doi = {10.31234/osf.io/zcf8s},
  abstract = {The use of statistical inference in linguistics and related areas like psychology typically involves a binary decision: either reject or accept some null hypothesis using statistical significance testing. When statistical power is low, this frequentist data-analytic approach breaks down: null results are uninformative, and significant results are driven by Type M error. Using an example from psycholinguistics, several alternative approaches are demonstrated for reporting inconsistencies between the data and a theoretical prediction. The key here is to focus on committing to a falsifiable prediction, on quantifying uncertainty statistically, and to learn to accept the fact that---in almost all practical data analysis situations---we can only draw uncertain conclusions from data, regardless of whether we manage to obtain statistical significance or not. A focus on uncertainty quantification is likely to lead to fewer excessively bold claims that, on closer investigation, may turn out to be not supported by the data},
  keywords = {Bayesian data analysis,experimental linguistics,Uncertainty quantification},
  file = {/Users/danielapalleschi/Zotero/storage/K4K2NELE/Vasishth_2021.pdf}
}


@book{wickham_r_2016-1,
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  shorttitle = {R for Data Science},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  abstract = {"This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience"--},
  isbn = {978-1-4919-1039-9 978-1-4919-1036-8},
  lccn = {QA276.45.R3 W53 2016},
  keywords = {Big data,Computer programs,Data mining,Databases,Electronic data processing,Information visualization,R (Computer program language),Statistics},
  annotation = {OCLC: ocn968213225}
}
@misc{isager_deciding_nodate,
  title = {Deciding What to Replicate: {{A}} Formal Definition of ``replication Value'' and a Decision Model for Replication Study Selection.},
  author = {Isager, Peder Mortvedt},
  year = {2020},
  publisher = {{MetaArXiv}},
  doi = {10. 1037/met0000438},
  abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, researchers who conduct replication studies face a difficult problem; there are many more studies in need of replication than there are funds available for replicating. To select studies for replication efficiently, we need to understand which studies are the most in need of replication. In other words, we need to understand which replication efforts have the highest expected utility. In this article we propose a general rule for study selection in replication research based on the replication value of the claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by conducting a replication of the claim, and is a function of (1) the value of being certain about the claim, and (2) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/8MR739YW/Isager - Deciding what to replicate A formal definition of.pdf}
}

@misc{isager_replication_2021,
  title = {Replication Value as a Function of Citation Impact and Sample Size},
  author = {Isager, Peder Mortvedt and Van 'T Veer, Anna Elisabeth and Lakens, Daniel},
  year = {2021},
  month = aug,
  doi = {10.31222/osf.io/knjea},
  urldate = {2024-04-18},
  abstract = {Researchers seeking to replicate original research often need to decide which of several relevant candidates to select for replication. Several strategies for study selection have been proposed, utilizing a variety of observed indicators as criteria for selection. However, few strategies clearly specify the goal of study selection and how that goal is related to the indicators that are utilized. We have previously formalized a decision model of replication study selection in which the goal of study selection is to maximize the expected utility gain of the replication effort. We further define the concept of replication value as a proxy for expected utility gain (Isager et al., 2020). In this article, we propose a quantitative operationalization of replication value. We first discuss how value and uncertainty - the two concepts used to determine replication value -- could be estimated via information about citation count and sample size. Second, we propose an equation for combining these indicators into an overall estimate of replication value, which we denote RVCn. Third, we suggest how RVCn could be implemented as part of a broader study selection procedure. Finally, we provide preliminary data suggesting that studies that were in fact selected for replication tend to have relatively high RVCn estimates. The goal of this article is to explain how RVCn is intended to work and, in doing so, demonstrate the many assumptions that should be explicit in any replication study selection strategy.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/legalcode},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/ECZSZSF6/Isager et al. - 2021 - Replication value as a function of citation impact.pdf}
}
@book{wickham_r_2023,
  title = {R for {{Data Science}}},
  author = {Wickham, Hadley and {\c C}etinkaya-Rundel, Mine and Grolemund, Garrett},
  year = {2023},
  edition = {2},
  file = {/Users/danielapalleschi/Zotero/storage/JIFX5NSK/Wickham et al. - 2023 - R for Data Science.pdf}
}


@book{baayen_analyzing_2008,
  title = {Analyzing {{Linguistic Data}}: {{A Practical Introduction}} to {{Statistics}} Using {{R}}},
  author = {Baayen, R. H.},
  year = {2008},
  abstract = {The Agricultural Ingeneer Dr. Teruo Higa, professor of Horticulture at the University of The Ryukyus in Okinawa, Japan creates a technique in the 80th de- cade related with the use of efficient microorganism. This technology is the basis of the present review aim at providing information about groups of benevolent microorganisms such as: lactic acid bacteria, photo- trophic bacteria, actinomycetes group, yeast group and fungi present in natural ecosystems which are physiologically compatible with each other. Efficient Microorganisms, as a microbial inoculantion, resto- re soil microbiological balance, improve its physical and chemical conditions, increase crop production and protection, preserve natural resources, and ge- nerate a more sustainable agriculture and environ- ment. They can be used in the livestock (cattle, por- ciculture and poultry) for animal husbandry and the increase of productive variables. All this maximizes the efficiency of the systems and the management of excreta and facilities.},
  isbn = {978-0-521-88259-0},
  keywords = {linguistics,psycholinguistics,R,statistics},
  file = {/Users/danielapalleschi/Zotero/storage/QFSVN9V7/Analyzing Linguistic Data A Practical Introduction to Statistics using R by R. H. Baayen (1).pdf}
}
@article{biondo_yesterday_2022,
  title = {Yesterday Is History, Tomorrow Is a Mystery: {{An}} Eye-Tracking Investigation of the Processing of Past and Future Time Reference during Sentence Reading.},
  shorttitle = {Yesterday Is History, Tomorrow Is a Mystery},
  author = {Biondo, Nicoletta and Soilemezidi, Marielena and Mancini, Simona},
  year = {2022},
  month = jul,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {48},
  number = {7},
  pages = {1001--1018},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0001053},
  urldate = {2023-03-02},
  abstract = {The ability to think about nonpresent time is a crucial aspect of human cognition. Both the past and future imply a temporal displacement of an event outside the ``now.'' They also intrinsically differ: The past refers to inalterable events; the future to alterable events, to possible worlds. Are the past and future processed similarly or differently? In this study, we addressed this question by investigating how Spanish speakers process past/future time reference violations during sentence processing, while recording eye movements. We also investigated the role of verbs (in isolation; within sentences) and adverbs (deictic; nondeictic) during time processing. Existing accounts propose that past processing, which requires a link to discourse, is more complex than future processing, which{\textemdash}like the present{\textemdash}is locally bound. Our findings show that past and future processing differs, especially at early stages of verb processing, but this difference is not limited to the presence/absence of discourse linking. We found earlier mismatch effects for past compared to future time reference in incongruous sentences, in line with previous studies. Interestingly, it took longer to categorize the past than the future tense when verbs were presented in isolation. However, it took longer to categorize the future than the past when verbs were presented in congruous sentences, arguably because the future implies alterable worlds. Finally, temporal adverbs were found to play an important role in reinspection and reanalysis triggered by the presence of undefined time frames (nondeictic adverbs) or incongruences (mismatching verbs).},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/AGNCU5MQ/Biondo et al. - 2022 - Yesterday is history, tomorrow is a mystery An ey.pdf}
}
@misc{winter_linear_2013,
  title = {Linear Models and Linear Mixed Effects Models in {{R}}: {{Tutorial}} 1},
  author = {Winter, Bodo},
  year = {2013},
  number = {arXiv:1308.5499},
  file = {/Users/danielapalleschi/Zotero/storage/GLT7NUL3/Winter_LME_tutorial1.pdf}
}

@misc{winter_very_2014,
  title = {A Very Basic Tutorial for Performing Linear Mixed Effects Analyses ({{Tutorial}} 2)},
  author = {Winter, Bodo},
  year = {2014},
  number = {arXiv:1308.5499},
  abstract = {This tutorial serves as a quick boot camp to jump-start your own analyses with linear mixed effects models. This text is different from other introductions by being decidedly conceptual; I will focus on why you want to use mixed models and how you should use them. While many introductions to this topic can be very daunting to readers who lake the appropriate statistical background, this text is going to be a softer kind of introduction{\ldots} so, don't panic! The tutorial requires R {\textendash} so if you haven't installed it yet, go and get it! I also recommend reading tutorial 1 in this series before you go further. You can find it here:},
  file = {/Users/danielapalleschi/Zotero/storage/4IG4XCA6/m-api-1e560dae-7aa7-7515-5c84-34d4445900b6.pdf}
}

@book{winter_statistics_2019,
  title = {Statistics for {{Linguists}}: {{An Introduction Using R}}},
  author = {Winter, Bodo},
  year = {2019},
  journal = {Statistics for Linguists: An Introduction Using R},
  publisher = {{Routledge}},
  doi = {10.4324/9781315165547},
  abstract = {Statistics for Linguists: An Introduction Using R is the first statistics textbook on linear models for linguistics. The book covers simple uses of linear models through generalized models to more advanced approaches, maintaining its focus on conceptual issues and avoiding excessive mathematical details. It contains many applied examples using the R statistical programming environment. Written in an accessible tone and style, this text is the ideal main resource for graduate and advanced undergraduate students of Linguistics statistics courses as well as those in other fields, including Psychology, Cognitive Science, and Data Science.},
  isbn = {978-1-138-05608-4},
  file = {/Users/danielapalleschi/Zotero/storage/DHWNSK68/Statistics for Linguists_ An Introduction - Bodo Winter.pdf}
}
@book{sonderegger_regression_2023,
  title = {Regression {{Modeling}} for {{Linguistic Data}}},
  author = {Sonderegger, Morgan},
  year = {2023},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/VTTDZ24D/Sonderegger - Regression Modeling for Linguistic Data.pdf}
}
@article{perkel_six_2023,
  title = {Six Tips for Better Coding with {{ChatGPT}}},
  author = {Perkel, Jeffrey M.},
  date = {2023-06-08},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {618},
  number = {7964},
  pages = {422--423},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/d41586-023-01833-0},
  url = {https://www.nature.com/articles/d41586-023-01833-0},
  urldate = {2024-02-14},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/VRL6PXDR/Perkel - 2023 - Six tips for better coding with ChatGPT.pdf}
}

@article{sonderegger_medium-term_2017,
  title = {The Medium-Term Dynamics of Accents on Reality Television},
  author = {Sonderegger, Morgan and Bane, Max and Graff, Peter},
  year = {2017},
  journal = {Language},
  volume = {93},
  number = {3},
  pages = {598--640},
  issn = {1535-0665},
  doi = {10.1353/lan.2017.0038},
  urldate = {2024-02-05},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/YFB5JW9V/Sonderegger et al. - 2017 - The medium-term dynamics of accents on reality tel.pdf}
}

@article{nordmann_data_2022,
  title = {Data {{Visualization Using R}} for {{Researchers Who Do Not Use R}}},
  author = {Nordmann, Emily and McAleer, Phil and Toivo, Wilhelmiina and Paterson, Helena and DeBruine, Lisa M.},
  year = {2022},
  month = apr,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {251524592210746},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/25152459221074654},
  urldate = {2022-11-01},
  abstract = {In addition to benefiting reproducibility and transparency, one of the advantages of using R is that researchers have a much larger range of fully customizable data visualizations options than are typically available in point-and-click software because of the open-source nature of R. These visualization options not only look attractive but also can increase transparency about the distribution of the underlying data rather than relying on commonly used visualizations of aggregations, such as bar charts of means. In this tutorial, we provide a practical introduction to data visualization using R specifically aimed at researchers who have little to no prior experience of using R. First, we detail the rationale for using R for data visualization and introduce the ``grammar of graphics'' that underlies data visualization using the ggplot package. The tutorial then walks the reader through how to replicate plots that are commonly available in pointand-click software, such as histograms and box plots, and shows how the code for these ``basic'' plots can be easily extended to less commonly available options, such as violin box plots. The data set and code used in this tutorial and an interactive version with activity solutions, additional resources, and advanced plotting options are available at https:// osf.io/bj83f/.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/ZKS32S5J/Nordmann et al. - 2022 - Data Visualization Using R for Researchers Who Do .pdf}
}
@article{wickham_welcome_2019,
  title = {Welcome to the {{Tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  issn = {2475-9066},
  doi = {10.21105/joss.01686},
  urldate = {2024-01-23},
  abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/VES74E9F/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf}
}

@Article{tinytex-package,
    title = {TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live},
    author = {Yihui Xie},
    journal = {TUGboat},
    year = {2019},
    volume = {40},
    number = {1},
    pages = {30--32},
    url = {https://tug.org/TUGboat/Contents/contents40-1.html},
  }
@Manual{datasauRus-package,
    title = {datasauRus: Datasets from the Datasaurus Dozen},
    author = {Rhian Davies and Steph Locke and Lucy {D'Agostino McGowan}},
    year = {2022},
    note = {R package version 0.1.6},
    url = {https://CRAN.R-project.org/package=datasauRus},
  }
@Manual{here-package,
    title = {here: A Simpler Way to Find Your Files},
    author = {Kirill Müller},
    year = {2020},
    note = {R package version 1.0.1},
    url = {https://CRAN.R-project.org/package=here},
  }
@Article{tidyverse-package,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
@Manual{languageR-package,
    title = {languageR: Analyzing Linguistic Data: A Practical Introduction to
Statistics},
    author = {R. H. Baayen and Elnaz Shafaei-Bajestan},
    year = {2019},
    note = {R package version 1.5.0},
    url = {https://CRAN.R-project.org/package=languageR},
  }
@Manual{xie_tinytex_2023,
       title = {tinytex: Helper Functions to Install and Maintain TeX Live, and Compile
LaTeX Documents},
    author = {Yihui Xie},
    year = {2023},
    note = {R package version 0.45},
    url = {https://github.com/rstudio/tinytex},
  }
@misc{nordmann_applied_2022,
  title = {Applied Data Skills},
  author = {Nordmann, Emily and DeBruine, Lisa},
  year = {2022},
  month = mar,
  doi = {10.5281/zenodo.6365078},
  howpublished = {Zenodo}
}
@Manual{languageR-package,
    title = {languageR: Analyzing Linguistic Data: A Practical Introduction to
Statistics},
    author = {R. H. Baayen and Elnaz Shafaei-Bajestan},
    year = {2019},
    note = {R package version 1.5.0},
    url = {https://CRAN.R-project.org/package=languageR},
  }
